{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ccbe09-183f-4383-94cc-1090453fae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text-unidecode\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: text-unidecode\n",
      "Successfully installed text-unidecode-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install text-unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f0ce4f-02a8-4936-9b45-d773492bea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from text_unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "import ast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228341d0-150a-42e8-abe0-15f5e088a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare Your Dataset ---\n",
    "# Make sure your CSV file is in the same directory or provide the full path.\n",
    "try:\n",
    "    df = pd.read_csv(\"Normalized_Dataset/AND_Normalized1.csv\")\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please replace 'your_dataset.csv' with the actual name of your data file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f47544f5-0f38-4490-a86a-bd7374f37ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new, guaranteed-unique 'publication_id' column.\n",
      "\n",
      "Verification successful: All column names are unique.\n"
     ]
    }
   ],
   "source": [
    "# --- THE CORE FIX: Guarantee a single, unique 'publication_id' ---\n",
    "# 1. Drop ALL potential old ID columns to prevent any conflicts.\n",
    "#    errors='ignore' ensures this runs even if a column doesn't exist.\n",
    "df = df.drop(columns=['publication_id', 'Paper_id', 'Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# 2. Reset the index. This creates a new 'index' column with unique integers (0, 1, 2...).\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "# 3. Rename the new, unique 'index' column to be our one and only 'publication_id'.\n",
    "df = df.rename(columns={'index': 'publication_id'})\n",
    "print(\"Created a new, guaranteed-unique 'publication_id' column.\")\n",
    "\n",
    "# --- RENAME COLUMNS and CLEAN DATA ---\n",
    "rename_map = {\n",
    "    'Full_Name': 'author_name', 'Publication Year': 'year', 'Collaborators': 'co_authors',\n",
    "    'Address': 'affiliation', 'journal': 'venue', 'title': 'title'\n",
    "}\n",
    "df = df.rename(columns=lambda c: rename_map.get(c, c))\n",
    "df['ground_truth_author_id'] = df['OID'].astype(str)\n",
    "nan_mask = df['ground_truth_author_id'].isin(['nan', 'None'])\n",
    "df.loc[nan_mask, 'ground_truth_author_id'] = [f'unknown_{i}' for i in range(nan_mask.sum())]\n",
    "\n",
    "key_text_cols = ['author_name', 'co_authors', 'affiliation', 'venue', 'title']\n",
    "for col in key_text_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('')\n",
    "if 'year' in df.columns:\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- NORMALIZE DATA ---\n",
    "def normalize_name(name):\n",
    "    if not isinstance(name, str) or not name: return \"\"\n",
    "    name = unidecode(name).lower()\n",
    "    name = re.sub(r'[^a-z\\s,]', '', name)\n",
    "    parts = [p.strip() for p in re.split(r'[\\s,]+', name) if p.strip()]\n",
    "    if not parts: return \"\"\n",
    "    lastname = parts[-1]; initials = [p[0] for p in parts[:-1]]\n",
    "    return \" \".join(initials) + \" \" + lastname\n",
    "\n",
    "def normalize_text_generic(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = unidecode(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_coauthors(coauthor_str):\n",
    "    if not isinstance(coauthor_str, str) or not coauthor_str: return []\n",
    "    try:\n",
    "        coauthor_list = ast.literal_eval(coauthor_str)\n",
    "        if isinstance(coauthor_list, list): return sorted([normalize_name(name) for name in coauthor_list])\n",
    "    except: return sorted([normalize_name(name) for name in coauthor_str.split(';')])\n",
    "    return []\n",
    "\n",
    "df_normalized = df.copy()\n",
    "df_normalized['norm_author_name'] = df['author_name'].apply(normalize_name)\n",
    "df_normalized['norm_co_authors'] = df['co_authors'].apply(normalize_coauthors)\n",
    "df_normalized['norm_title'] = df['title'].apply(normalize_text_generic)\n",
    "df_normalized['norm_venue'] = df['venue'].apply(normalize_text_generic)\n",
    "df_normalized['norm_affiliation'] = df['affiliation'].apply(normalize_text_generic)\n",
    "\n",
    "# --- VERIFY THE FIX ---\n",
    "assert df_normalized.columns.is_unique, \"FATAL ERROR: Duplicate columns were still created.\"\n",
    "print(\"\\nVerification successful: All column names are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ac9df9-ff60-4ca7-961a-b95c1c0f739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55429 entries, 0 to 55428\n",
      "Data columns (total 29 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   publication_id          55429 non-null  int64  \n",
      " 1   SurName                 55426 non-null  object \n",
      " 2   GivenNames              55392 non-null  object \n",
      " 3   author_name             55429 non-null  object \n",
      " 4   Emailid                 55429 non-null  object \n",
      " 5   Caveats_email           55429 non-null  bool   \n",
      " 6   CorsAu                  55429 non-null  bool   \n",
      " 7   OID                     45614 non-null  object \n",
      " 8   RID                     41662 non-null  object \n",
      " 9   co_authors              55429 non-null  object \n",
      " 10  Publication Month       55429 non-null  float64\n",
      " 11  year                    55429 non-null  int64  \n",
      " 12  Subject_cat             55429 non-null  object \n",
      " 13  affiliation             55429 non-null  object \n",
      " 14  AuNames                 55429 non-null  object \n",
      " 15  Country                 54619 non-null  object \n",
      " 16  Keyword                 55429 non-null  object \n",
      " 17  title                   55429 non-null  object \n",
      " 18  venue                   55429 non-null  object \n",
      " 19  KeywordPlus             55429 non-null  object \n",
      " 20  UID                     55429 non-null  object \n",
      " 21  SurName_phonetic        0 non-null      float64\n",
      " 22  GivenNames_phonetic     0 non-null      float64\n",
      " 23  ground_truth_author_id  55429 non-null  object \n",
      " 24  norm_author_name        55429 non-null  object \n",
      " 25  norm_co_authors         55429 non-null  object \n",
      " 26  norm_title              55429 non-null  object \n",
      " 27  norm_venue              55429 non-null  object \n",
      " 28  norm_affiliation        55429 non-null  object \n",
      "dtypes: bool(2), float64(3), int64(2), object(22)\n",
      "memory usage: 11.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_normalized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3982377-688b-4df1-ac7c-cc2e61ae15da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting blocking and pair generation...\n",
      "Blocking and pair generation complete.\n",
      "\n",
      "Generated 491020 candidate pairs for comparison.\n",
      "Example pairs: [(7747, 7749), (16623, 16624), (46843, 47445), (34051, 34058), (18585, 18627)]\n",
      "\n",
      "--- Pipeline Complete: Successfully set 'publication_id' as the index. ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# BLOCK 2: BLOCKING AND CANDIDATE PAIR GENERATION\n",
    "# ==============================================================================\n",
    "\n",
    "def create_blocks_and_pairs(df_norm):\n",
    "    print(\"\\nStarting blocking and pair generation...\")\n",
    "    df_temp = df_norm[['publication_id', 'norm_author_name']].copy()\n",
    "    def get_block_key(name):\n",
    "        if isinstance(name, str) and ' ' in name:\n",
    "            parts = name.split()\n",
    "            return f\"{parts[-1]} {parts[0][0]}\"\n",
    "        return None\n",
    "    df_temp['block_key'] = df_temp['norm_author_name'].apply(get_block_key)\n",
    "    df_temp = df_temp.dropna(subset=['block_key'])\n",
    "\n",
    "    grouped = df_temp.groupby('block_key')['publication_id'].apply(list)\n",
    "    blocks = grouped[grouped.str.len() > 1]\n",
    "\n",
    "    candidate_pairs = set()\n",
    "    for pub_ids_list in blocks:\n",
    "        for i in range(len(pub_ids_list)):\n",
    "            for j in range(i + 1, len(pub_ids_list)):\n",
    "                id1, id2 = sorted((pub_ids_list[i], pub_ids_list[j]))\n",
    "                candidate_pairs.add((id1, id2))\n",
    "    print(\"Blocking and pair generation complete.\")\n",
    "    return list(candidate_pairs)\n",
    "\n",
    "candidate_pairs = create_blocks_and_pairs(df_normalized)\n",
    "print(f\"\\nGenerated {len(candidate_pairs)} candidate pairs for comparison.\")\n",
    "if len(candidate_pairs) > 5:\n",
    "    print(\"Example pairs:\", candidate_pairs[:5])\n",
    "else:\n",
    "    print(\"Example pairs:\", candidate_pairs)\n",
    "\n",
    "# --- FINAL STEP: SET INDEX ---\n",
    "# This will now succeed because the DataFrame is correctly structured.\n",
    "df_normalized = df_normalized.set_index('publication_id', drop=False)\n",
    "print(\"\\n--- Pipeline Complete: Successfully set 'publication_id' as the index. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60c8d7e4-32c4-44d2-8bcc-adb4c5c563b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\author name disambiguation\\venv\\lib\\site-packages (1.7.2)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in d:\\author name disambiguation\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in d:\\author name disambiguation\\venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.14.2-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\author name disambiguation\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\author name disambiguation\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\author name disambiguation\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\author name disambiguation\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in d:\\author name disambiguation\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\author name disambiguation\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\author name disambiguation\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\author name disambiguation\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\author name disambiguation\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\author name disambiguation\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\author name disambiguation\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: anyio in d:\\author name disambiguation\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\author name disambiguation\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\author name disambiguation\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\author name disambiguation\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\author name disambiguation\\venv\\lib\\site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 12.7 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading torch-2.9.0-cp312-cp312-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 72.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 34.1/109.3 MB 83.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 51.6/109.3 MB 84.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 62.7/109.3 MB 75.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 77.6/109.3 MB 73.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 86.2/109.3 MB 68.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 94.6/109.3 MB 63.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 100.9/109.3 MB 59.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  108.0/109.3 MB 57.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 56.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 56.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 56.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.1/109.3 MB 56.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 109.3/109.3 MB 39.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 41.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 28.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading rapidfuzz-3.14.2-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, rapidfuzz, networkx, fsspec, filelock, torch, lightgbm, Levenshtein, huggingface-hub, tokenizers, python-Levenshtein, transformers, sentence-transformers\n",
      "Successfully installed Levenshtein-0.27.1 filelock-3.20.0 fsspec-2025.10.0 huggingface-hub-0.36.0 lightgbm-4.6.0 mpmath-1.3.0 networkx-3.5 python-Levenshtein-0.27.1 rapidfuzz-3.14.2 safetensors-0.6.2 sentence-transformers-5.1.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 transformers-4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn lightgbm sentence-transformers python-Levenshtein networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "630f69b6-a5e6-4332-9d1f-522408e570a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SBERT model...\n",
      "\n",
      "Computing SBERT embeddings for all titles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9aa08806bb4def9156535fc8bf55c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for candidate pairs...\n",
      "\n",
      "Created feature matrix of shape: (491020, 6)\n",
      "Positive pairs: 220318 | Negative pairs: 270702\n",
      "[LightGBM] [Info] Number of positive: 154223, number of negative: 189491\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 812\n",
      "[LightGBM] [Info] Number of data points in the train set: 343714, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448696 -> initscore=-0.205942\n",
      "[LightGBM] [Info] Start training from score -0.205942\n",
      "\n",
      "Classifier training complete.\n",
      "\n",
      "Classifier Performance Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90     81211\n",
      "           1       0.81      1.00      0.90     66095\n",
      "\n",
      "    accuracy                           0.90    147306\n",
      "   macro avg       0.91      0.91      0.90    147306\n",
      "weighted avg       0.91      0.90      0.90    147306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Author Name Disambiguation\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import lightgbm as lgb\n",
    "import Levenshtein\n",
    "from Levenshtein import jaro_winkler as jarowinkler # Correct import/alias of the function\n",
    "import networkx as nx\n",
    "import numpy as np # <-- Added: Missing numpy import\n",
    "\n",
    "# Initialize the SBERT model once\n",
    "print(\"\\nLoading SBERT model...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Note: The 'jarowinkler' function is now the direct Jaro-Winkler similarity function.\n",
    "# ----------------------------------------------------------------------\n",
    "def compute_features(pub1, pub2, sbert_embeddings):\n",
    "    \"\"\"Computes a feature vector for a pair of publications (adapted for your data).\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Name Similarity (Jaro-Winkler)\n",
    "    # FIX: Directly call the aliased function 'jarowinkler'\n",
    "    features['name_jaro'] = jarowinkler(pub1['norm_author_name'], pub2['norm_author_name'])\n",
    "\n",
    "    # Affiliation Similarity (Token Jaccard)\n",
    "    aff1 = set(pub1['norm_affiliation'].split())\n",
    "    aff2 = set(pub2['norm_affiliation'].split())\n",
    "    features['aff_jaccard'] = len(aff1.intersection(aff2)) / len(aff1.union(aff2)) if aff1.union(aff2) else 0\n",
    "\n",
    "    # Co-author Similarity (Jaccard)\n",
    "    # Note: Assuming 'norm_co_authors' is a list/set of co-author names\n",
    "    coauth1 = set(pub1['norm_co_authors'])\n",
    "    coauth2 = set(pub2['norm_co_authors'])\n",
    "    features['coauth_jaccard'] = len(coauth1.intersection(coauth2)) / len(coauth1.union(coauth2)) if coauth1.union(coauth2) else 0\n",
    "    \n",
    "    # Venue (Journal) Similarity (Normalized Levenshtein)\n",
    "    venue1, venue2 = pub1['norm_venue'], pub2['norm_venue']\n",
    "    max_len = max(len(venue1), len(venue2))\n",
    "    # Levenshtein distance normalized by the maximum length\n",
    "    features['venue_lev'] = 1 - (Levenshtein.distance(venue1, venue2) / max_len) if max_len > 0 else 1\n",
    "\n",
    "    # Temporal Proximity (Exponential Decay)\n",
    "    year_diff = abs(pub1['year'] - pub2['year'])\n",
    "    features['year_prox'] = np.exp(-0.1 * year_diff)\n",
    "\n",
    "    # Title Semantic Similarity (SBERT)\n",
    "    emb1 = sbert_embeddings[pub1.name] # .name gives the index (publication_id)\n",
    "    emb2 = sbert_embeddings[pub2.name]\n",
    "    # cosine_similarity expects 2D arrays, hence the list wrappers\n",
    "    features['title_sbert_sim'] = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --- Prepare data for ML model ---\n",
    "print(\"\\nComputing SBERT embeddings for all titles...\")\n",
    "# Assuming df_normalized is available and has 'title' and 'publication_id' as index\n",
    "title_embeddings = sbert_model.encode(df_normalized['title'].tolist(), show_progress_bar=True)\n",
    "# Create a mapping from publication_id to its embedding\n",
    "sbert_map = dict(zip(df_normalized.index, title_embeddings))\n",
    "\n",
    "print(\"\\nEngineering features for candidate pairs...\")\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Assuming candidate_pairs is available as a list of tuples: [(id1, id2), ...]\n",
    "for id1, id2 in candidate_pairs:\n",
    "    pub1 = df_normalized.loc[id1]\n",
    "    pub2 = df_normalized.loc[id2]\n",
    "    \n",
    "    # Check if ground_truth_author_id is a Series (happens if index is not unique)\n",
    "    # If loc returns a DataFrame, take the first row (common issue with non-unique indices)\n",
    "    if isinstance(pub1, pd.DataFrame):\n",
    "         pub1 = pub1.iloc[0]\n",
    "         pub2 = pub2.iloc[0]\n",
    "    \n",
    "    features = compute_features(pub1, pub2, sbert_map)\n",
    "    X.append(list(features.values()))\n",
    "    \n",
    "    # Assuming 'ground_truth_author_id' is accessible\n",
    "    label = 1 if pub1['ground_truth_author_id'] == pub2['ground_truth_author_id'] else 0\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"\\nCreated feature matrix of shape: {X.shape}\")\n",
    "print(f\"Positive pairs: {np.sum(y)} | Negative pairs: {len(y) - np.sum(y)}\")\n",
    "\n",
    "# --- Train the Fusion Model (LightGBM) ---\n",
    "if len(X) > 0 and np.sum(y) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    lgb_classifier = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "    lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nClassifier training complete.\")\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred = lgb_classifier.predict(X_test)\n",
    "    print(\"\\nClassifier Performance Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"\\nSkipping model training due to insufficient data or lack of positive samples.\")\n",
    "    lgb_classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa8e24d-9f0e-4b9e-8530-a55a3f5bd360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Author Name Disambiguation\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constructing similarity graph...\n",
      "Graph constructed with 55429 nodes and 270354 edges.\n",
      "\n",
      "Detecting communities (clusters)...\n",
      "\n",
      "Final Disambiguation Results (Sample):\n",
      "                       author_name               ground_truth_author_id  \\\n",
      "publication_id                                                            \n",
      "0                    akhavian reza   akhavian, reza/0000-0001-9691-8016   \n",
      "1                    akhavian reza   akhavian, reza/0000-0001-9691-8016   \n",
      "4               budhitama subagdja                            unknown_2   \n",
      "2               budhitama subagdja                            unknown_0   \n",
      "3               budhitama subagdja                            unknown_1   \n",
      "6                          feng yu         feng, yu/0000-0001-6433-5035   \n",
      "5                          feng yu         feng, yu/0000-0001-6433-5035   \n",
      "7                   mansour nasser  mansour, nasser/0000-0001-5707-7373   \n",
      "8                   mansour nasser  mansour, nasser/0000-0001-5707-7373   \n",
      "9                   banerjee samik  banerjee, samik/0000-0003-2325-1489   \n",
      "\n",
      "                predicted_cluster_id  \n",
      "publication_id                        \n",
      "0                                  0  \n",
      "1                                  0  \n",
      "4                                  1  \n",
      "2                                  1  \n",
      "3                                  1  \n",
      "6                                  2  \n",
      "5                                  2  \n",
      "7                                  3  \n",
      "8                                  3  \n",
      "9                                  4  \n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "if lgb_classifier and len(candidate_pairs) > 0:\n",
    "    # --- Step 1: Construct the Learned Similarity Graph ---\n",
    "    print(\"\\nConstructing similarity graph...\")\n",
    "    pair_probabilities = lgb_classifier.predict_proba(X)[:, 1]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df_normalized.index)\n",
    "\n",
    "    for i, (id1, id2) in enumerate(candidate_pairs):\n",
    "        prob = pair_probabilities[i]\n",
    "        if prob > 0.5: # Use a 50% probability threshold to create an edge\n",
    "            G.add_edge(id1, id2, weight=prob)\n",
    "            \n",
    "    print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "    # --- Step 2 & 3: Community Detection ---\n",
    "    # We will use a single, robust method: Connected Components\n",
    "    # This is simple and effective. Louvain is better for graphs with complex community structure.\n",
    "    print(\"\\nDetecting communities (clusters)...\")\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    \n",
    "    # --- Assigning Cluster IDs ---\n",
    "    final_clusters = {}\n",
    "    cluster_id_counter = 0\n",
    "    for component in clusters:\n",
    "        for node in component:\n",
    "            final_clusters[node] = cluster_id_counter\n",
    "        cluster_id_counter += 1\n",
    "\n",
    "    # Handle publications that were not in any pair (singletons)\n",
    "    all_nodes = set(df_normalized.index)\n",
    "    clustered_nodes = set(final_clusters.keys())\n",
    "    singleton_nodes = all_nodes - clustered_nodes\n",
    "    for node in singleton_nodes:\n",
    "        final_clusters[node] = cluster_id_counter\n",
    "        cluster_id_counter += 1\n",
    "        \n",
    "    df_normalized['predicted_cluster_id'] = df_normalized.index.map(final_clusters)\n",
    "    \n",
    "    print(\"\\nFinal Disambiguation Results (Sample):\")\n",
    "    print(df_normalized[['author_name', 'ground_truth_author_id', 'predicted_cluster_id']].sort_values('predicted_cluster_id').head(10))\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping clustering because the model was not trained.\")\n",
    "    df_normalized['predicted_cluster_id'] = range(len(df_normalized)) # Assign all to unique clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db30d26f-f955-425a-9530-f38e178fa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation ---\n",
      "Normalized Mutual Information (NMI) Score: 0.9823\n",
      "B-Cubed Precision: 0.8589\n",
      "B-Cubed Recall: 0.9984\n",
      "B-Cubed F1-Score: 0.9234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "if lgb_classifier and len(candidate_pairs) > 0:\n",
    "    gt_labels = df_normalized['ground_truth_author_id'].values\n",
    "    pred_labels = df_normalized['predicted_cluster_id'].values\n",
    "\n",
    "    nmi_score = normalized_mutual_info_score(gt_labels, pred_labels)\n",
    "    print(f\"\\n--- Evaluation ---\")\n",
    "    print(f\"Normalized Mutual Information (NMI) Score: {nmi_score:.4f}\")\n",
    "\n",
    "    def b_cubed_score(true_labels_df, pred_labels_df):\n",
    "        true_map = true_labels_df.groupby(true_labels_df).groups\n",
    "        pred_map = pred_labels_df.groupby(pred_labels_df).groups\n",
    "        \n",
    "        precision_sum = 0.0\n",
    "        recall_sum = 0.0\n",
    "        \n",
    "        for item_idx in true_labels_df.index:\n",
    "            true_cluster_label = true_labels_df[item_idx]\n",
    "            pred_cluster_label = pred_labels_df[item_idx]\n",
    "            \n",
    "            true_cluster = set(true_map[true_cluster_label])\n",
    "            pred_cluster = set(pred_map[pred_cluster_label])\n",
    "\n",
    "            intersection_size = len(true_cluster.intersection(pred_cluster))\n",
    "            \n",
    "            precision_sum += intersection_size / len(pred_cluster)\n",
    "            recall_sum += intersection_size / len(true_cluster)\n",
    "            \n",
    "        p = precision_sum / len(true_labels_df)\n",
    "        r = recall_sum / len(true_labels_df)\n",
    "        f1 = (2 * p * r) / (p + r) if (p + r) > 0 else 0\n",
    "        return p, r, f1\n",
    "\n",
    "    precision, recall, f1 = b_cubed_score(\n",
    "        df_normalized['ground_truth_author_id'],\n",
    "        df_normalized['predicted_cluster_id']\n",
    "    )\n",
    "    print(f\"B-Cubed Precision: {precision:.4f}\")\n",
    "    print(f\"B-Cubed Recall: {recall:.4f}\")\n",
    "    print(f\"B-Cubed F1-Score: {f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fe63876-a111-408b-a24d-f99d4486b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM classifier...\n",
      "[LightGBM] [Info] Number of positive: 165238, number of negative: 203027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 810\n",
      "[LightGBM] [Info] Number of data points in the train set: 368265, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448693 -> initscore=-0.205952\n",
      "[LightGBM] [Info] Start training from score -0.205952\n",
      "Training complete.\n",
      "LightGBM model saved to and_model.pkl\n",
      "SentenceTransformer model saved to the 'sbert_model/' directory.\n",
      "Normalized publication data saved to publication_database.csv\n",
      "--- Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# ==============================================================================\n",
    "# BLOCK 3: MODEL TRAINING AND SAVING\n",
    "# ==============================================================================\n",
    "if len(X) > 0 and np.sum(y) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    lgb_classifier = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "    print(\"Training LightGBM classifier...\")\n",
    "    lgb_classifier.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- SAVE THE MODELS ---\n",
    "    # Save the trained LightGBM model\n",
    "    joblib.dump(lgb_classifier, 'and_model.pkl')\n",
    "    print(\"LightGBM model saved to and_model.pkl\")\n",
    "\n",
    "    # The SentenceTransformer model saves itself as a folder\n",
    "    sbert_model.save('sbert_model')\n",
    "    print(\"SentenceTransformer model saved to the 'sbert_model/' directory.\")\n",
    "    \n",
    "    # Save the full normalized data frame, which we will use as our database\n",
    "    df_normalized.to_csv('publication_database.csv')\n",
    "    print(\"Normalized publication data saved to publication_database.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping model training and saving due to insufficient data.\")\n",
    "\n",
    "print(\"--- Pipeline Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b790be-5377-40dd-bb87-bce73e87b413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
